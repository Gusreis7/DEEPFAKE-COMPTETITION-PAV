{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio import transforms\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoModel\n",
    "\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "                            AutoModelForAudioClassification,\n",
    "                            EarlyStoppingCallback,\n",
    "                            AutoFeatureExtractor,\n",
    "                            TrainingArguments,\n",
    "                            Trainer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor\n",
    "from transformers import AutoProcessor, Wav2Vec2Model,Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Info = namedtuple(\"Info\", [\"length\", \"sample_rate\", \"channels\"])\n",
    "def get_audio_info(path: str):\n",
    "    info = torchaudio.info(path)\n",
    "    if hasattr(info, 'num_frames'):\n",
    "        return Info(info.num_frames, info.sample_rate, info.num_channels)\n",
    "    else:\n",
    "        siginfo = info[0]\n",
    "        return Info(siginfo.length // siginfo.channels, siginfo.rate, siginfo.channels)\n",
    "\n",
    "\n",
    "def get_total_dataset_length(base_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Gets information related to the length of the audio data\n",
    "    \"\"\"\n",
    "    length = []\n",
    "    srs = []\n",
    "    channels = []\n",
    "\n",
    "    file_paths = glob.glob(os.path.join(base_dir, '**', '*.mp3'), recursive=True)\n",
    "    fails = 0\n",
    "    fail_paths = []\n",
    "    for file_path in tqdm.tqdm(file_paths):\n",
    "        try:\n",
    "          audio_info = get_audio_info(file_path)\n",
    "          srs.append(audio_info[1])\n",
    "          channels.append(audio_info[2])\n",
    "          length.append(audio_info[0]/audio_info[1])\n",
    "        except Exception as e:\n",
    "          fails+=1\n",
    "          fail_paths.append(file_path)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Min audio length (in seconds): {min(length)} | Max audio length (in seconds): {max(length)}\")\n",
    "    print(f\"Mean audio length (in seconds): {np.mean(length)} | Median: {np.median(length)} | Std: {np.std(length)}\")\n",
    "    print(f\"Total amount of data (in minutes): {np.sum(length)/60}\")\n",
    "    print('-'*50)\n",
    "    print(f\"Different sample rates in audios in the dataset: {set(srs)}\")\n",
    "    print(f\"Different number of channels in the audios in the dataset: {set(channels)}\")\n",
    "    print(f\"Falhas na contagem {fails}/{len(file_paths)}\")\n",
    "    for fail_audio in fail_paths:\n",
    "      print(fail_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_total_dataset_length(\"../audios/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,sr = torchaudio.load('../audios/train/fake/ff79bf6b-f3e5-480d-b7c7-a36c0a1ddf46.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeatures():\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
    "        self.model = Wav2Vec2Model.from_pretrained(model_path).to(DEVICE)\n",
    "    \n",
    "    def extract_features(self,speech_array, target_sample_rate):\n",
    "        input_tensor = self.processor(speech_array, sampling_rate=target_sample_rate, return_tensors='pt', padding=True).input_values\n",
    "        input_tensor = torch.squeeze(input_tensor,dim=0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(input_tensor).last_hidden_state\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrained_ExtractFeatures():\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.processor = AutoFeatureExtractor.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path).to(DEVICE)\n",
    "    \n",
    "    def extract_features(self,speech_array, target_sample_rate):\n",
    "        input_tensor = self.processor(speech_array, sampling_rate=target_sample_rate, return_tensors='pt', padding=True).input_values\n",
    "        input_tensor = torch.squeeze(input_tensor,dim=1).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(input_tensor).last_hidden_state\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AST_ExtractFeatures():\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.extractor = AutoFeatureExtractor.from_pretrained(model_path)\n",
    "        self.model = ASTForAudioClassification.from_pretrained(model_path, output_hidden_states=True).to(DEVICE)\n",
    "    \n",
    "    def extract_features(self,speech_array, sample_rate):\n",
    "         \n",
    "        input_tensor = self.extractor(speech_array, sampling_rate=sample_rate, return_tensors='pt').input_values.to(DEVICE)\n",
    "        input_tensor = input_tensor.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(input_tensor).hidden_states\n",
    "        return embeddings[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, is_label, TARGET_SAMPLE_RATE = 16000, mode = 'mel_spec', feature_extractor = None, ls= None):\n",
    "        #is label = True para audios com label, False para audios sem label, isso garante que mudanças no pipeline de extração de features sejam para ambos os conjuntos\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = [\"real\", \"fake\"]\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.is_label = is_label\n",
    "        self.TARGET_SAMPLE_RATE =TARGET_SAMPLE_RATE\n",
    "        self.mode = mode\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.label_smoothing = ls\n",
    "        if self.is_label:\n",
    "            for class_idx, class_name in enumerate(self.classes):\n",
    "                class_dir = os.path.join(data_dir, class_name)\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith(\".mp3\"):\n",
    "                        self.audio_files.append(os.path.join(class_dir, file))\n",
    "                        self.labels.append(class_idx)\n",
    "        else:            \n",
    "            for file in os.listdir(self.data_dir):\n",
    "                if file.endswith(\".mp3\"):\n",
    "                    self.audio_files.append(os.path.join(self.data_dir,file))\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=TARGET_SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]\n",
    "        if self.is_label:\n",
    "            label = self.labels[idx]\n",
    "            if self.label_smoothing != None:\n",
    "                if label == 1: \n",
    "                    label = (1-self.label_smoothing) +(self.label_smoothing/2)\n",
    "                if label == 0:\n",
    "                    label = self.label_smoothing/2\n",
    "\n",
    "\n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(audio_file)\n",
    "        # Convert to mono\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = torch.mean(audio, dim=0).unsqueeze(0)\n",
    "\n",
    "        if sr != self.TARGET_SAMPLE_RATE:\n",
    "            audio = torchaudio.transforms.Resample(sr, self.TARGET_SAMPLE_RATE)(audio)\n",
    "\n",
    "        # Pad or truncate the audio to a fixed length\n",
    "        fixed_length = (\n",
    "            self.TARGET_SAMPLE_RATE * 4\n",
    "        )  # Adjust this value based on your requirements\n",
    "        if audio.shape[1] < fixed_length:\n",
    "            audio = torch.nn.functional.pad(audio, (0, fixed_length - audio.shape[1]))\n",
    "        else:\n",
    "            audio = audio[:, :fixed_length]\n",
    "        \n",
    "        if self.mode == 'mel_spec':\n",
    "            audio = self.mel_spectrogram(audio)\n",
    "            if self.is_label:\n",
    "                return audio, label\n",
    "            else:\n",
    "                #import for test generating\n",
    "                return audio, os.path.basename(audio_file)\n",
    "                \n",
    "        elif self.mode == 'feature':\n",
    "            audio = self.feature_extractor.extract_features(audio.squeeze(dim=0), self.TARGET_SAMPLE_RATE)\n",
    "            if self.is_label:\n",
    "                return audio, label\n",
    "            else:\n",
    "                #import for test generating\n",
    "                return audio, os.path.basename(audio_file)\n",
    "            \n",
    "        else:\n",
    "            print(\"This feature extraction wasnt implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "   \n",
    "    def __init__(self, batch_size,dataset_train,dataset_test, do_split):\n",
    "        self.modes = ['train','test']\n",
    "        self.dataloaders = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.do_split = do_split\n",
    "        if self.do_split:\n",
    "            self.modes = ['train','validation','test']\n",
    "            generator = torch.Generator().manual_seed(42)\n",
    "            train_size = int(len(dataset_train.audio_files)*0.8)\n",
    "            val_size = int(len(dataset_train.audio_files)-train_size)\n",
    "            train_set, val_set = random_split(dataset_train, [train_size, val_size], generator=generator)\n",
    "\n",
    "            self.dataloaders['train'] = train_set\n",
    "            self.dataloaders['validation'] = val_set\n",
    "        else:\n",
    "            self.dataloaders['train'] = dataset_train\n",
    "            \n",
    "        self.dataloaders['test'] = dataset_test\n",
    "    \n",
    "\n",
    "    def get_loader(self, mode):\n",
    "        if mode == 'train':\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:   \n",
    "    def __init__(self):\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def get_loss(self, y, y_hat):\n",
    "        return self.loss_fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        dim  = self.flatten(torch.zeros(self.input_size).unsqueeze(0)).shape[1]\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(16*20*18,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv_layer(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, input_size):\n",
    "        self.model = NeuralNetwork(input_size=input_size)\n",
    "        self.model.to(DEVICE)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.nweights = 0\n",
    "        for name,weights in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                self.nweights = self.nweights + weights.numel()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def update(self, loss):\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def l1_loss(self):\n",
    "        L1_term = torch.tensor(0., requires_grad=True)\n",
    "        \n",
    "        for name, weights in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                weights_sum = torch.sum(torch.abs(weights))\n",
    "                L1_term = L1_term + weights_sum\n",
    "        L1_term = L1_term / self.nweights\n",
    "        return L1_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics e best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    #FOR TRAIN AND VALIDATION ONLY\n",
    "    def __init__(self):\n",
    "        self.metrics_save = {}\n",
    "        self.best_models_weigths = {}\n",
    "        self.metrics_names  = ['acuracy','recall','precision','f1-score']\n",
    "        \n",
    "    def calc_metrics(self,preds,labels,mode,loss, model_weigths=None, show=False):\n",
    "        acc = accuracy_score(y_pred=preds, y_true=labels)\n",
    "        recall = recall_score(y_pred=preds, y_true=labels)\n",
    "        precision = precision_score(y_pred=preds, y_true=labels)\n",
    "        f1 = f1_score(y_pred=preds, y_true=labels,average='binary')\n",
    "        metrics_values = [acc,recall,precision,f1]\n",
    "\n",
    "\n",
    "        ###################LOSS#############################################\n",
    "        if f'{mode}_loss' not in self.metrics_save.keys():\n",
    "            self.metrics_save[f'{mode}_loss'] = [loss]\n",
    "        else:\n",
    "            self.metrics_save[f'{mode}_loss'].append(loss)\n",
    "        ################################################################\n",
    "        \n",
    "        if show:\n",
    "            print(f\"{mode} -  Acuracy: {acc:.2f} - Recall {recall:.2f} - Precision {precision:.2f} - F1-Score {f1:.2f} - Loss {loss:.2f}\")\n",
    "\n",
    "\n",
    "        #############Metrics##################################################\n",
    "        \n",
    "        for metric_name, metric_value in zip(self.metrics_names, metrics_values):\n",
    "\n",
    "            ###Add metrics \n",
    "            if f'{mode}_{metric_name}' not in self.metrics_save.keys():\n",
    "                self.metrics_save[f'{mode}_{metric_name}'] = [metric_value]\n",
    "            else:\n",
    "                self.metrics_save[f'{mode}_{metric_name}'].append(metric_value)\n",
    "            \n",
    "            ###Sava best metrics and respective weigths \n",
    "            if mode == 'train':\n",
    "                if f'{mode}_best_{metric_name}' not in self.metrics_save.keys():\n",
    "                    self.metrics_save[f'{mode}_best_{metric_name}'] = metric_value\n",
    "                    self.best_models_weigths[f'{mode}_best_{metric_name}'] = model_weigths\n",
    "                elif metric_value > self.metrics_save[f'{mode}_best_{metric_name}'] :\n",
    "                    self.best_models_weigths[f'{mode}_best_{metric_name}'] = model_weigths\n",
    "    \n",
    "        ################################################################\n",
    "\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        metrics = self.metrics_names.copy()\n",
    "        modes = ['train','validation']\n",
    "        for i,metric_name in enumerate(metrics):\n",
    "            metric_train  = self.metrics_save[f'{modes[0]}_{metric_name}']\n",
    "            metric_validation  = self.metrics_save[f'{modes[1]}_{metric_name}']\n",
    "\n",
    "            ii = i % 2\n",
    "            jj = i // 2\n",
    "            axs[jj,ii].plot(metric_train, label=modes[0])\n",
    "            axs[jj,ii].plot(metric_validation, label=modes[1])\n",
    "\n",
    "            axs[jj,ii].set_title(f'{metric_name}: {modes[0]} x {modes[1]}')\n",
    "            axs[jj,ii].set_xlabel('Epoch')\n",
    "            axs[jj,ii].set_ylabel(f'{metric_name}')\n",
    "            axs[jj,ii].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Mostrar o plot\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        modes = ['train','validation']\n",
    "        metric_name = 'loss'\n",
    "        metric_train  = (self.metrics_save[f'{modes[0]}_{metric_name}'])\n",
    "        metric_validation  = (self.metrics_save[f'{modes[1]}_{metric_name}'])\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        axs.set_title(f'Log scale {metric_name}: {modes[0]} x {modes[1]}')\n",
    "        axs.plot(metric_train, label=modes[0])\n",
    "        axs.plot(metric_validation, label=modes[1])\n",
    "        axs.set_xlabel('Epoch')\n",
    "        axs.set_ylabel(f'{metric_name}')\n",
    "        axs.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def get_best_model(self, metric):\n",
    "        for key in self.best_models_weigths.keys():\n",
    "                if metric in key:\n",
    "                    return self.best_models_weigths[key]\n",
    "    \n",
    "    def save_best_model(self,all_metrics, metric='F1-Score'):\n",
    "        if all_metrics:\n",
    "            print(\"Saving all models\")\n",
    "            for key in self.best_models_weigths.keys():\n",
    "                torch.save(f'{self.best_models_weigths[key]}.pt', key)\n",
    "                print(f\"Save model at: {key}.pt\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Saving best model for {metric}\")\n",
    "            for key in self.best_models_weigths.keys():\n",
    "                if metric in key:\n",
    "                    torch.save(f'{self.best_models_weigths[key]}.pt', key)\n",
    "                    print(f\"Save model at: {key}.pt\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data: Data, learner: Learner, evaluator: Evaluator, metrics: Metrics):\n",
    "        self.data = data\n",
    "        self.learner = learner\n",
    "        self.metrics = metrics\n",
    "        self.evaluator = evaluator\n",
    "\n",
    "    def one_epoch(self, mode):\n",
    "        if mode == 'train':\n",
    "            self.learner.model.train(True)\n",
    "        else:\n",
    "            self.learner.model.train(False)\n",
    "    \n",
    "\n",
    "        dataloader = self.data.get_loader(mode)\n",
    "        preds = []\n",
    "        labels = []\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for (X, y) in tqdm.tqdm(dataloader):\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "            y_hat = self.learner.predict(X)\n",
    "            \n",
    "            loss = self.evaluator.get_loss(y, y_hat)\n",
    "               \n",
    "            if mode == 'train':\n",
    "                #L1_term  = self.learner.l1_loss()\n",
    "                #loss  = loss - (L1_term * 0.1)\n",
    "                self.learner.update(loss)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            labels.extend(y.int().tolist())\n",
    "            preds.extend((y_hat > 0.5).int().tolist())\n",
    "        \n",
    "        epoch_loss /= len(dataloader)\n",
    "\n",
    "        #preds,labels,mode,loss, model_weigths=None, show=False\n",
    "        self.metrics.calc_metrics(preds=preds, labels=labels, mode=mode, loss=epoch_loss, model_weigths=self.learner.model.state_dict(), show=True)\n",
    "\n",
    "    def test(self,mode,name_test, model_weigths=None):\n",
    "        self.learner.model.load_state_dict(model_weigths)\n",
    "        self.learner.model.train(False)\n",
    "        dataloader = self.data.get_loader(mode)\n",
    "        preds = []\n",
    "        ids = []\n",
    "        for (X, x_id) in tqdm.tqdm(dataloader):\n",
    "            X = X.to(DEVICE)\n",
    "            y_hat = self.learner.predict(X)\n",
    "            ids.extend(x_id)\n",
    "            preds.extend((y_hat).float().tolist())\n",
    "        \n",
    "        file_test_submtion =  open(f'{name_test}.csv','w')\n",
    "        file_test_submtion.write('id,fake_prob\\n')\n",
    "        for idx,pred in zip(ids,preds):\n",
    "            file_test_submtion.write(f\"{idx},{pred[0]}\\n\")\n",
    "        print(f\"Test submission for {name_test} saved at {name_test}.csv\")\n",
    "\n",
    "\n",
    "    def run(self, n_epochs: int):\n",
    "        print(\"Starting training\")\n",
    "        for t in range(n_epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            self.one_epoch(mode='train')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(mode='validation')\n",
    "        print(\"Training done\")\n",
    "        \n",
    "    def run_test(self,name_test):\n",
    "        #Keep test at the end of training\n",
    "        metric = 'f1-score'\n",
    "        print(f\"Generating test probs with {metric} best model:\")\n",
    "        with torch.no_grad():\n",
    "            best_model = self.metrics.get_best_model(metric=metric)\n",
    "            self.test(mode='test', name_test=name_test, model_weigths=best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor = ExtractFeatures(model_path='../w2v_training/models/wav2vec2-basew2vls0.35')\n",
    "pre_feature_extractor = Pretrained_ExtractFeatures(model_path='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/w2v_training/models/wav2vec2-basew2vls0.35')\n",
    "#ast_feature_extractor = AST_ExtractFeatures(model_path='MIT/ast-finetuned-audioset-10-10-0.4593')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets\n",
    "audio_train = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/train', is_label=True, mode='feature', feature_extractor=pre_feature_extractor)\n",
    "audio_teste = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/test', is_label=False, mode='feature', feature_extractor=pre_feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "data =Data(batch_size=50, dataset_train=audio_train, dataset_test=audio_teste, do_split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner\n",
    "learner = Learner(input_size=audio_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data=data, evaluator=evaluator, learner=learner, metrics=metrics)\n",
    "trainer.run(n_epochs=5)\n",
    "trainer.run_test(name_test='test_pretrained_w2v_features_crossloss_extractor_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all metrics\n",
    "metrics.save_best_model(all_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
