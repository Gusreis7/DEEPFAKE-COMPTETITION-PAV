{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio import transforms\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/anaconda3/envs/study/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "                            AutoModelForAudioClassification,\n",
    "                            EarlyStoppingCallback,\n",
    "                            AutoFeatureExtractor,\n",
    "                            TrainingArguments,\n",
    "                            Trainer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor\n",
    "from transformers import AutoProcessor, Wav2Vec2Model,Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Info = namedtuple(\"Info\", [\"length\", \"sample_rate\", \"channels\"])\n",
    "def get_audio_info(path: str):\n",
    "    info = torchaudio.info(path)\n",
    "    if hasattr(info, 'num_frames'):\n",
    "        return Info(info.num_frames, info.sample_rate, info.num_channels)\n",
    "    else:\n",
    "        siginfo = info[0]\n",
    "        return Info(siginfo.length // siginfo.channels, siginfo.rate, siginfo.channels)\n",
    "\n",
    "\n",
    "def get_total_dataset_length(base_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Gets information related to the length of the audio data\n",
    "    \"\"\"\n",
    "    length = []\n",
    "    srs = []\n",
    "    channels = []\n",
    "\n",
    "    file_paths = glob.glob(os.path.join(base_dir, '**', '*.mp3'), recursive=True)\n",
    "    fails = 0\n",
    "    fail_paths = []\n",
    "    for file_path in tqdm.tqdm(file_paths):\n",
    "        try:\n",
    "          audio_info = get_audio_info(file_path)\n",
    "          srs.append(audio_info[1])\n",
    "          channels.append(audio_info[2])\n",
    "          length.append(audio_info[0]/audio_info[1])\n",
    "        except Exception as e:\n",
    "          fails+=1\n",
    "          fail_paths.append(file_path)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Min audio length (in seconds): {min(length)} | Max audio length (in seconds): {max(length)}\")\n",
    "    print(f\"Mean audio length (in seconds): {np.mean(length)} | Median: {np.median(length)} | Std: {np.std(length)}\")\n",
    "    print(f\"Total amount of data (in minutes): {np.sum(length)/60}\")\n",
    "    print('-'*50)\n",
    "    print(f\"Different sample rates in audios in the dataset: {set(srs)}\")\n",
    "    print(f\"Different number of channels in the audios in the dataset: {set(channels)}\")\n",
    "    print(f\"Falhas na contagem {fails}/{len(file_paths)}\")\n",
    "    for fail_audio in fail_paths:\n",
    "      print(fail_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_total_dataset_length(\"../audios/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,sr = torchaudio.load('../audios/train/fake/ff79bf6b-f3e5-480d-b7c7-a36c0a1ddf46.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeatures():\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
    "        self.model = Wav2Vec2Model.from_pretrained(model_path).to(DEVICE)\n",
    "    \n",
    "    def w2v_extract_features(self,speech_array, target_sample_rate):\n",
    "        input_tensor = self.processor(speech_array, sampling_rate=target_sample_rate, return_tensors='pt', padding=True).input_values\n",
    "        input_tensor = torch.squeeze(input_tensor,dim=0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(input_tensor).last_hidden_state\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, is_label, TARGET_SAMPLE_RATE = 16000, mode = 'mel_spec', feature_extractor = None):\n",
    "        #is label = True para audios com label, False para audios sem label, isso garante que mudanças no pipeline de extração de features sejam para ambos os conjuntos\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = [\"real\", \"fake\"]\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.is_label = is_label\n",
    "        self.TARGET_SAMPLE_RATE =TARGET_SAMPLE_RATE\n",
    "        self.mode = mode\n",
    "        self.feature_extractor = feature_extractor\n",
    "        if self.is_label:\n",
    "            for class_idx, class_name in enumerate(self.classes):\n",
    "                class_dir = os.path.join(data_dir, class_name)\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith(\".mp3\"):\n",
    "                        self.audio_files.append(os.path.join(class_dir, file))\n",
    "                        self.labels.append(class_idx)\n",
    "        else:            \n",
    "            for file in os.listdir(self.data_dir):\n",
    "                if file.endswith(\".mp3\"):\n",
    "                    self.audio_files.append(os.path.join(self.data_dir,file))\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=TARGET_SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]\n",
    "        if self.is_label:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(audio_file)\n",
    "        # Convert to mono\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = torch.mean(audio, dim=0).unsqueeze(0)\n",
    "\n",
    "        if sr != self.TARGET_SAMPLE_RATE:\n",
    "            audio = torchaudio.transforms.Resample(sr, self.TARGET_SAMPLE_RATE)(audio)\n",
    "\n",
    "        # Pad or truncate the audio to a fixed length\n",
    "        fixed_length = (\n",
    "            self.TARGET_SAMPLE_RATE * 4\n",
    "        )  # Adjust this value based on your requirements\n",
    "        if audio.shape[1] < fixed_length:\n",
    "            audio = torch.nn.functional.pad(audio, (0, fixed_length - audio.shape[1]))\n",
    "        else:\n",
    "            audio = audio[:, :fixed_length]\n",
    "        \n",
    "        if self.mode == 'mel_spec':\n",
    "            audio = self.mel_spectrogram(audio)\n",
    "            if self.is_label:\n",
    "                return audio, label\n",
    "            else:\n",
    "                #import for test generating\n",
    "                return audio, os.path.basename(audio_file)\n",
    "                \n",
    "        elif self.mode == 'w2v_feature':\n",
    "            audio = self.feature_extractor.w2v_extract_features(audio, self.TARGET_SAMPLE_RATE)\n",
    "            if self.is_label:\n",
    "                return audio, label\n",
    "            else:\n",
    "                #import for test generating\n",
    "                return audio, os.path.basename(audio_file)\n",
    "        else:\n",
    "            print(\"This feature extraction wasnt implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "   \n",
    "    def __init__(self, batch_size,dataset_train,dataset_test, do_split):\n",
    "        self.modes = ['train','test']\n",
    "        self.dataloaders = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.do_split = do_split\n",
    "        if self.do_split:\n",
    "            self.modes = ['train','validation','test']\n",
    "            generator = torch.Generator().manual_seed(42)\n",
    "            train_size = int(len(dataset_train.audio_files)*0.8)\n",
    "            val_size = int(len(dataset_train.audio_files)-train_size)\n",
    "            train_set, val_set = random_split(dataset_train, [train_size, val_size], generator=generator)\n",
    "\n",
    "            self.dataloaders['train'] = train_set\n",
    "            self.dataloaders['validation'] = val_set\n",
    "        else:\n",
    "            self.dataloaders['train'] = dataset_train\n",
    "            \n",
    "        self.dataloaders['test'] = dataset_test\n",
    "    \n",
    "\n",
    "    def get_loader(self, mode):\n",
    "        if mode == 'train':\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "    def get_loss(self, y, y_hat):\n",
    "        return self.loss_fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        dim  = self.flatten(torch.zeros(self.input_size).unsqueeze(0)).shape[1]\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.Linear(128,32),\n",
    "            nn.Linear(32,4),\n",
    "            nn.Linear(4,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv_layer(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, input_size):\n",
    "        self.model = NeuralNetwork(input_size=input_size)\n",
    "        self.model.to(DEVICE)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.nweights = 0\n",
    "        for name,weights in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                self.nweights = self.nweights + weights.numel()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def update(self, loss):\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def l1_loss(self):\n",
    "        L1_term = torch.tensor(0., requires_grad=True)\n",
    "        \n",
    "        for name, weights in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                weights_sum = torch.sum(torch.abs(weights))\n",
    "                L1_term = L1_term + weights_sum\n",
    "        L1_term = L1_term / self.nweights\n",
    "        return L1_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics e best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    #FOR TRAIN AND VALIDATION ONLY\n",
    "    def __init__(self):\n",
    "        self.metrics_save = {}\n",
    "        self.best_models_weigths = {}\n",
    "        self.metrics_names  = ['acuracy','recall','precision','f1-score']\n",
    "        \n",
    "    def calc_metrics(self,preds,labels,mode,loss, model_weigths=None, show=False):\n",
    "        acc = accuracy_score(y_pred=preds, y_true=labels)\n",
    "        recall = recall_score(y_pred=preds, y_true=labels)\n",
    "        precision = precision_score(y_pred=preds, y_true=labels)\n",
    "        f1 = f1_score(y_pred=preds, y_true=labels,average='binary')\n",
    "        metrics_values = [acc,recall,precision,f1]\n",
    "\n",
    "\n",
    "        ###################LOSS#############################################\n",
    "        if f'{mode}_loss' not in self.metrics_save.keys():\n",
    "            self.metrics_save[f'{mode}_loss'] = [loss]\n",
    "        else:\n",
    "            self.metrics_save[f'{mode}_loss'].append(loss)\n",
    "        ################################################################\n",
    "        \n",
    "        if show:\n",
    "            print(f\"{mode} -  Acuracy: {acc:.2f} - Recall {recall:.2f} - Precision {precision:.2f} - F1-Score {f1:.2f} - Loss {loss:.2f}\")\n",
    "\n",
    "\n",
    "        #############Metrics##################################################\n",
    "        \n",
    "        for metric_name, metric_value in zip(self.metrics_names, metrics_values):\n",
    "\n",
    "            ###Add metrics \n",
    "            if f'{mode}_{metric_name}' not in self.metrics_save.keys():\n",
    "                self.metrics_save[f'{mode}_{metric_name}'] = [metric_value]\n",
    "            else:\n",
    "                self.metrics_save[f'{mode}_{metric_name}'].append(metric_value)\n",
    "            \n",
    "            ###Sava best metrics and respective weigths \n",
    "            if mode == 'train':\n",
    "                if f'{mode}_best_{metric_name}' not in self.metrics_save.keys():\n",
    "                    self.metrics_save[f'{mode}_best_{metric_name}'] = metric_value\n",
    "                    self.best_models_weigths[f'{mode}_best_{metric_name}'] = model_weigths\n",
    "                elif metric_value > self.metrics_save[f'{mode}_best_{metric_name}'] :\n",
    "                    self.best_models_weigths[f'{mode}_best_{metric_name}'] = model_weigths\n",
    "    \n",
    "        ################################################################\n",
    "\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        metrics = self.metrics_names.copy()\n",
    "        modes = ['train','validation']\n",
    "        for i,metric_name in enumerate(metrics):\n",
    "            metric_train  = self.metrics_save[f'{modes[0]}_{metric_name}']\n",
    "            metric_validation  = self.metrics_save[f'{modes[1]}_{metric_name}']\n",
    "\n",
    "            ii = i % 2\n",
    "            jj = i // 2\n",
    "            axs[jj,ii].plot(metric_train, label=modes[0])\n",
    "            axs[jj,ii].plot(metric_validation, label=modes[1])\n",
    "\n",
    "            axs[jj,ii].set_title(f'{metric_name}: {modes[0]} x {modes[1]}')\n",
    "            axs[jj,ii].set_xlabel('Epoch')\n",
    "            axs[jj,ii].set_ylabel(f'{metric_name}')\n",
    "            axs[jj,ii].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Mostrar o plot\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        modes = ['train','validation']\n",
    "        metric_name = 'loss'\n",
    "        metric_train  = np.exp(self.metrics_save[f'{modes[0]}_{metric_name}'])\n",
    "        metric_validation  = np.exp(self.metrics_save[f'{modes[1]}_{metric_name}'])\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        axs.set_title(f'Log scale {metric_name}: {modes[0]} x {modes[1]}')\n",
    "        axs.plot(metric_train, label=modes[0])\n",
    "        axs.plot(metric_validation, label=modes[1])\n",
    "        axs.set_xlabel('Epoch')\n",
    "        axs.set_ylabel(f'{metric_name}')\n",
    "        axs.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def get_best_model(self, metric):\n",
    "        for key in self.best_models_weigths.keys():\n",
    "                if metric in key:\n",
    "                    return self.best_models_weigths[key]\n",
    "    \n",
    "    def save_best_model(self,all_metrics, metric='F1-Score'):\n",
    "        if all_metrics:\n",
    "            print(\"Saving all models\")\n",
    "            for key in self.best_models_weigths.keys():\n",
    "                torch.save(f'{self.best_models_weigths[key]}.pt', key)\n",
    "                print(f\"Save model at: {key}.pt\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Saving best model for {metric}\")\n",
    "            for key in self.best_models_weigths.keys():\n",
    "                if metric in key:\n",
    "                    torch.save(f'{self.best_models_weigths[key]}.pt', key)\n",
    "                    print(f\"Save model at: {key}.pt\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data: Data, learner: Learner, evaluator: Evaluator, metrics: Metrics):\n",
    "        self.data = data\n",
    "        self.learner = learner\n",
    "        self.metrics = metrics\n",
    "        self.evaluator = evaluator\n",
    "\n",
    "    def one_epoch(self, mode):\n",
    "        if mode == 'train':\n",
    "            self.learner.model.train(True)\n",
    "        else:\n",
    "            self.learner.model.train(False)\n",
    "    \n",
    "\n",
    "        dataloader = self.data.get_loader(mode)\n",
    "        preds = []\n",
    "        labels = []\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for (X, y) in tqdm.tqdm(dataloader):\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "            y_hat = self.learner.predict(X)\n",
    "            \n",
    "            loss = self.evaluator.get_loss(y, y_hat)\n",
    "               \n",
    "            if mode == 'train':\n",
    "                #L1_term  = self.learner.l1_loss()\n",
    "                #loss  = loss - (L1_term * 0.1)\n",
    "                self.learner.update(loss)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            labels.extend(y.int().tolist())\n",
    "            preds.extend((y_hat > 0.5).int().tolist())\n",
    "        \n",
    "        epoch_loss /= len(dataloader)\n",
    "\n",
    "        #preds,labels,mode,loss, model_weigths=None, show=False\n",
    "        self.metrics.calc_metrics(preds=preds, labels=labels, mode=mode, loss=epoch_loss, model_weigths=self.learner.model.state_dict(), show=True)\n",
    "\n",
    "    def test(self,mode,name_test, model_weigths=None):\n",
    "        self.learner.model.load_state_dict(model_weigths)\n",
    "        self.learner.model.train(False)\n",
    "        dataloader = self.data.get_loader(mode)\n",
    "        preds = []\n",
    "        ids = []\n",
    "        for (X, x_id) in tqdm.tqdm(dataloader):\n",
    "            X = X.to(DEVICE)\n",
    "            y_hat = self.learner.predict(X)\n",
    "            ids.extend(x_id)\n",
    "            preds.extend((y_hat).float().tolist())\n",
    "        \n",
    "        file_test_submtion =  open(f'{name_test}.csv','w')\n",
    "        file_test_submtion.write('id,fake_prob\\n')\n",
    "        for idx,pred in zip(ids,preds):\n",
    "            file_test_submtion.write(f\"{idx},{pred[0]}\\n\")\n",
    "        print(f\"Test submission for {name_test} saved at {name_test}.csv\")\n",
    "\n",
    "\n",
    "    def run(self, n_epochs: int):\n",
    "        print(\"Starting training\")\n",
    "        for t in range(n_epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            self.one_epoch(mode='train')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(mode='validation')\n",
    "        print(\"Training done\")\n",
    "        \n",
    "    def run_test(self,name_test):\n",
    "        #Keep test at the end of training\n",
    "        metric = 'f1-score'\n",
    "        print(f\"Generating test probs with {metric} best model:\")\n",
    "        with torch.no_grad():\n",
    "            best_model = self.metrics.get_best_model(metric=metric)\n",
    "            self.test(mode='test', name_test=name_test, model_weigths=best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/anaconda3/envs/study/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ExtractFeatures(model_path='facebook/wav2vec2-base-960h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets\n",
    "audio_train = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/train', is_label=True, mode='w2v_feature', feature_extractor=feature_extractor)\n",
    "audio_teste = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/test', is_label=False, mode='w2v_feature', feature_extractor=feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "data =Data(batch_size=50, dataset_train=audio_train, dataset_test=audio_teste, do_split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/anaconda3/envs/study/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0011, -0.0034, -0.0303,  ..., -0.1873,  0.0598,  0.0644],\n",
       "          [-0.0004, -0.0032, -0.0312,  ..., -0.1857,  0.0591,  0.0635],\n",
       "          [ 0.0026, -0.0018, -0.0247,  ..., -0.1900,  0.0579,  0.0656],\n",
       "          ...,\n",
       "          [-0.0220, -0.0068, -0.0404,  ..., -0.1929,  0.0527,  0.0872],\n",
       "          [-0.0232, -0.0076, -0.0426,  ..., -0.1938,  0.0519,  0.0883],\n",
       "          [-0.0259, -0.0103, -0.0424,  ..., -0.1894,  0.0519,  0.0850]]],\n",
       "        device='cuda:0'),\n",
       " 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 46715240448 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#learner\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m learner \u001b[38;5;241m=\u001b[39m \u001b[43mLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mLearner.__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mNeuralNetwork.__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mFlatten()\n\u001b[1;32m     12\u001b[0m dim  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_relu_stack \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     15\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(dim\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m,dim\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 46715240448 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "#learner\n",
    "learner = Learner(input_size=audio_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data=data, evaluator=evaluator, learner=learner, metrics=metrics)\n",
    "trainer.run(n_epochs=30)\n",
    "trainer.run_test(name_test='test_w2v_features_extractor_dropout')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all metrics\n",
    "metrics.save_best_model(all_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
