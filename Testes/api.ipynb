{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch import nn\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "import librosa\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, is_label, TARGET_SAMPLE_RATE = 16000):\n",
    "        #is label = True para audios com label, False para audios sem label, isso garante que mudanças no pipeline de extração de features sejam para ambos os conjuntos\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = [\"real\", \"fake\"]\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.is_label = is_label\n",
    "        self.TARGET_SAMPLE_RATE =TARGET_SAMPLE_RATE\n",
    "        if self.is_label:\n",
    "            for class_idx, class_name in enumerate(self.classes):\n",
    "                class_dir = os.path.join(data_dir, class_name)\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith(\".mp3\"):\n",
    "                        self.audio_files.append(os.path.join(class_dir, file))\n",
    "                        self.labels.append(class_idx)\n",
    "        else:            \n",
    "            for file in os.listdir(self.data_dir):\n",
    "                if file.endswith(\".mp3\"):\n",
    "                    self.audio_files.append(os.path.join(self.data_dir,file))\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=TARGET_SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]\n",
    "        if self.is_label:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(audio_file)\n",
    "        # Convert to mono\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = torch.mean(audio, dim=0).unsqueeze(0)\n",
    "\n",
    "        if sr != self.TARGET_SAMPLE_RATE:\n",
    "            audio = torchaudio.transforms.Resample(sr, self.TARGET_SAMPLE_RATE)(audio)\n",
    "\n",
    "        # Pad or truncate the audio to a fixed length\n",
    "        fixed_length = (\n",
    "            self.TARGET_SAMPLE_RATE * 3\n",
    "        )  # Adjust this value based on your requirements\n",
    "        if audio.shape[1] < fixed_length:\n",
    "            audio = torch.nn.functional.pad(audio, (0, fixed_length - audio.shape[1]))\n",
    "        else:\n",
    "            audio = audio[:, :fixed_length]\n",
    "\n",
    "        audio = self.mel_spectrogram(audio)\n",
    "        if self.is_label:\n",
    "            return audio, label\n",
    "        else:\n",
    "            return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[5.0537e-04, 5.8895e-04, 9.6446e-04,  ..., 1.7757e-03,\n",
       "           3.6543e-03, 3.7238e-04],\n",
       "          [1.6781e-04, 1.3125e-03, 2.7401e-04,  ..., 9.0541e-03,\n",
       "           9.1673e-03, 2.1658e-03],\n",
       "          [5.7434e-04, 6.7541e-04, 1.3042e-04,  ..., 2.0726e-02,\n",
       "           1.6678e-02, 4.8293e-03],\n",
       "          ...,\n",
       "          [1.0396e-07, 2.0007e-13, 1.1095e-13,  ..., 4.1733e-10,\n",
       "           1.3565e-10, 1.3352e-07],\n",
       "          [1.0269e-07, 1.2887e-13, 1.3506e-13,  ..., 7.3700e-10,\n",
       "           3.2287e-10, 1.3118e-07],\n",
       "          [1.0342e-07, 1.5312e-13, 1.2068e-13,  ..., 3.7980e-09,\n",
       "           1.5164e-09, 1.3202e-07]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_train = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/train', is_label=True)\n",
    "audio_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.7918e-03, 2.0068e-02, 4.0089e-03,  ..., 2.7534e-02,\n",
       "          6.5274e-03, 1.3242e-01],\n",
       "         [2.5826e-03, 6.3364e-02, 3.4908e-02,  ..., 5.4090e-02,\n",
       "          4.8813e-02, 1.4507e-01],\n",
       "         [5.7900e-03, 3.0583e-02, 4.7984e-02,  ..., 2.1832e+01,\n",
       "          8.3534e+00, 1.1720e+01],\n",
       "         ...,\n",
       "         [2.2706e-07, 8.7983e-06, 2.0074e-03,  ..., 2.7007e-02,\n",
       "          3.6072e-03, 3.8843e-03],\n",
       "         [2.8828e-08, 3.2975e-10, 2.3564e-08,  ..., 5.4293e-03,\n",
       "          2.0038e-03, 4.7438e-03],\n",
       "         [1.7535e-07, 5.8559e-08, 7.8749e-11,  ..., 1.3839e-02,\n",
       "          2.6288e-03, 5.2364e-03]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_teste = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/test', is_label=False)\n",
    "audio_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "   \n",
    "    def __init__(self, batch_size,dataset_train,dataset_test, do_split):\n",
    "        self.modes = ['train','test']\n",
    "        self.dataloaders = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.do_split = do_split\n",
    "        if self.do_split:\n",
    "            self.modes = ['train','validation','test']\n",
    "            generator = torch.Generator().manual_seed(42)\n",
    "            train_size = int(len(dataset_train.audio_files)*0.8)\n",
    "            val_size = int(len(dataset_train.audio_files)-train_size)\n",
    "            train_set, val_set = random_split(dataset_train, [train_size, val_size], generator=generator)\n",
    "\n",
    "            self.dataloaders['train'] = train_set\n",
    "            self.dataloaders['validation'] = val_set\n",
    "        else:\n",
    "            self.dataloaders['train'] = dataset_train\n",
    "            \n",
    "        self.dataloaders['test'] = dataset_test\n",
    "    \n",
    "\n",
    "    def get_loader(self, mode):\n",
    "        if mode == 'train':\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_teste = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/test', is_label=False) #test\n",
    "audio_train = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/train', is_label=True) #train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =Data(batch_size=100, dataset_train=audio_train, dataset_test=audio_teste, do_split=False)\n",
    "\n",
    "loader_train = data.get_loader('train')\n",
    "loader_test = data.get_loader('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =Data(batch_size=100, dataset_train=audio_train, dataset_test=audio_teste, do_split=True)\n",
    "\n",
    "loader_train = data.get_loader('train')\n",
    "loader_validation = data.get_loader('validation')\n",
    "loader_test = data.get_loader('test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
