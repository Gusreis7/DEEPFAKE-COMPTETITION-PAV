{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, is_label, TARGET_SAMPLE_RATE = 16000):\n",
    "        #is label = True para audios com label, False para audios sem label, isso garante que mudanças no pipeline de extração de features sejam para ambos os conjuntos\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = [\"real\", \"fake\"]\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.is_label = is_label\n",
    "        self.TARGET_SAMPLE_RATE =TARGET_SAMPLE_RATE\n",
    "        if self.is_label:\n",
    "            for class_idx, class_name in enumerate(self.classes):\n",
    "                class_dir = os.path.join(data_dir, class_name)\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith(\".mp3\"):\n",
    "                        self.audio_files.append(os.path.join(class_dir, file))\n",
    "                        self.labels.append(class_idx)\n",
    "        else:            \n",
    "            for file in os.listdir(self.data_dir):\n",
    "                if file.endswith(\".mp3\"):\n",
    "                    self.audio_files.append(os.path.join(self.data_dir,file))\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=TARGET_SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]\n",
    "        if self.is_label:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(audio_file)\n",
    "        # Convert to mono\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = torch.mean(audio, dim=0).unsqueeze(0)\n",
    "\n",
    "        if sr != self.TARGET_SAMPLE_RATE:\n",
    "            audio = torchaudio.transforms.Resample(sr, self.TARGET_SAMPLE_RATE)(audio)\n",
    "\n",
    "        # Pad or truncate the audio to a fixed length\n",
    "        fixed_length = (\n",
    "            self.TARGET_SAMPLE_RATE * 3\n",
    "        )  # Adjust this value based on your requirements\n",
    "        if audio.shape[1] < fixed_length:\n",
    "            audio = torch.nn.functional.pad(audio, (0, fixed_length - audio.shape[1]))\n",
    "        else:\n",
    "            audio = audio[:, :fixed_length]\n",
    "\n",
    "        audio = self.mel_spectrogram(audio)\n",
    "        if self.is_label:\n",
    "            return audio, label\n",
    "        else:\n",
    "            #import for test generating\n",
    "            return audio, os.path.basename(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "   \n",
    "    def __init__(self, batch_size,dataset_train,dataset_test, do_split):\n",
    "        self.modes = ['train','test']\n",
    "        self.dataloaders = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.do_split = do_split\n",
    "        if self.do_split:\n",
    "            self.modes = ['train','validation','test']\n",
    "            generator = torch.Generator().manual_seed(42)\n",
    "            train_size = int(len(dataset_train.audio_files)*0.8)\n",
    "            val_size = int(len(dataset_train.audio_files)-train_size)\n",
    "            train_set, val_set = random_split(dataset_train, [train_size, val_size], generator=generator)\n",
    "\n",
    "            self.dataloaders['train'] = train_set\n",
    "            self.dataloaders['validation'] = val_set\n",
    "        else:\n",
    "            self.dataloaders['train'] = dataset_train\n",
    "            \n",
    "        self.dataloaders['test'] = dataset_test\n",
    "    \n",
    "\n",
    "    def get_loader(self, mode):\n",
    "        if mode == 'train':\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            return  DataLoader(self.dataloaders[mode], batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "    def get_loss(self, y, y_hat):\n",
    "        return self.loss_fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        dummy_tensor = self.conv_layer(torch.zeros(self.input_size).unsqueeze(0))\n",
    "        dim = 1\n",
    "        for d in dummy_tensor.shape[1:]:\n",
    "            dim *= d\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, input_size):\n",
    "        self.model = NeuralNetwork(input_size=input_size)\n",
    "        self.model.to(DEVICE)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def update(self, loss):\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics e best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    #FOR TRAIN AND VALIDATION ONLY\n",
    "    def __init__(self):\n",
    "        self.metrics_save = {}\n",
    "        self.best_models_weigths = {}\n",
    "        self.metrics_names  = ['acuracy','recall','precision','f1-score']\n",
    "        \n",
    "    def calc_metrics(self,preds,labels,mode,loss, model_weigths=None, show=False):\n",
    "        acc = accuracy_score(y_pred=preds, y_true=labels)\n",
    "        recall = recall_score(y_pred=preds, y_true=labels)\n",
    "        precision = precision_score(y_pred=preds, y_true=labels)\n",
    "        f1 = f1_score(y_pred=preds, y_true=labels,average='binary')\n",
    "        metrics_values = [acc,recall,precision,f1]\n",
    "\n",
    "\n",
    "        ###################LOSS#############################################\n",
    "        if f'{mode}_loss' not in self.metrics_save.keys():\n",
    "            self.metrics_save[f'{mode}_loss'] = [loss]\n",
    "        else:\n",
    "            self.metrics_save[f'{mode}_loss'].append(loss)\n",
    "        ################################################################\n",
    "        \n",
    "        if show:\n",
    "            print(f\"{mode} -  Acuracy: {acc:.2f} - Recall {recall:.2f} - Precision {precision:.2f} - F1-Score {f1:.2f} - Loss {loss:.2f}\")\n",
    "\n",
    "\n",
    "        #############Metrics##################################################\n",
    "        \n",
    "        for metric_name, metric_value in zip(self.metrics_names, metrics_values):\n",
    "\n",
    "            ###Add metrics \n",
    "            if f'{mode}_{metric_name}' not in self.metrics_save.keys():\n",
    "                self.metrics_save[f'{mode}_{metric_name}'] = [metric_value]\n",
    "            else:\n",
    "                self.metrics_save[f'{mode}_{metric_name}'].append(metric_value)\n",
    "            \n",
    "            ###Sava best metrics and respective weigths \n",
    "            if mode == 'train':\n",
    "                if f'{mode}_best_{metric_name}' not in self.metrics_save.keys():\n",
    "                    self.metrics_save[f'{mode}_best_{metric_name}'] = metric_value\n",
    "                    self.best_models_weigths[f'{mode}_best_{metric_name}'] = model_weigths\n",
    "                elif metric_value > self.metrics_save[f'{mode}_best_{metric_name}'] :\n",
    "                    self.best_models_weigths[f'{mode}_best_{metric_name}'] = model_weigths\n",
    "    \n",
    "        ################################################################\n",
    "\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        metrics = self.metrics_names.copy()\n",
    "        modes = ['train','validation']\n",
    "        for i,metric_name in enumerate(metrics):\n",
    "            metric_train  = self.metrics_save[f'{modes[0]}_{metric_name}']\n",
    "            metric_validation  = self.metrics_save[f'{modes[1]}_{metric_name}']\n",
    "\n",
    "            ii = i % 2\n",
    "            jj = i // 2\n",
    "            axs[jj,ii].plot(metric_train, label=modes[0])\n",
    "            axs[jj,ii].plot(metric_validation, label=modes[1])\n",
    "\n",
    "            axs[jj,ii].set_title(f'{metric_name}: {modes[0]} x {modes[1]}')\n",
    "            axs[jj,ii].set_xlabel('Epoch')\n",
    "            axs[jj,ii].set_ylabel(f'{metric_name}')\n",
    "            axs[jj,ii].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Mostrar o plot\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        modes = ['train','validation']\n",
    "        metric_name = 'loss'\n",
    "        metric_train  = np.exp(self.metrics_save[f'{modes[0]}_{metric_name}'])\n",
    "        metric_validation  = np.exp(self.metrics_save[f'{modes[1]}_{metric_name}'])\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(6, 12))\n",
    "        i = 0\n",
    "        axs[i,i].set_title(f'Log scale {metric_name}: {modes[0]} x {modes[1]}')\n",
    "        axs[i,i].plot(metric_train, label=modes[0])\n",
    "        axs[i,i].plot(metric_validation, label=modes[1])\n",
    "        axs[i,i].set_xlabel('Epoch')\n",
    "        axs[i,i].set_ylabel(f'{metric_name}')\n",
    "        axs[i,i].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Mostrar o plot\n",
    "        plt.show()\n",
    "\n",
    "    def get_best_model(self, metric):\n",
    "        for key in self.best_models_weigths.keys():\n",
    "                if metric in key:\n",
    "                    return self.best_models_weigths[key]\n",
    "    \n",
    "    def save_best_model(self,all_metrics, metric='F1-Score'):\n",
    "        if all_metrics:\n",
    "            print(\"Saving all models\")\n",
    "            for key in self.best_models_weigths.keys():\n",
    "                torch.save(f'{self.best_models_weigths[key]}.pt', key)\n",
    "                print(f\"Save model at: {key}.pt\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Saving best model for {metric}\")\n",
    "            for key in self.best_models_weigths.keys():\n",
    "                if metric in key:\n",
    "                    torch.save(f'{self.best_models_weigths[key]}.pt', key)\n",
    "                    print(f\"Save model at: {key}.pt\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data: Data, learner: Learner, evaluator: Evaluator, metrics: Metrics):\n",
    "        self.data = data\n",
    "        self.learner = learner\n",
    "        self.metrics = metrics\n",
    "        self.evaluator = evaluator\n",
    "\n",
    "    def one_epoch(self, mode):\n",
    "        if mode == 'train':\n",
    "            self.learner.model.train(True)\n",
    "        else:\n",
    "            self.learner.model.train(False)\n",
    "    \n",
    "\n",
    "        dataloader = self.data.get_loader(mode)\n",
    "        preds = []\n",
    "        labels = []\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for (X, y) in tqdm.tqdm(dataloader):\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "            y_hat = self.learner.predict(X)\n",
    "            \n",
    "            loss = self.evaluator.get_loss(y, y_hat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if mode == 'train':\n",
    "                self.learner.update(loss)\n",
    "\n",
    "            labels.extend(y.int().tolist())\n",
    "            preds.extend((y_hat > 0.5).int().tolist())\n",
    "        \n",
    "        epoch_loss /= len(dataloader)\n",
    "\n",
    "        #preds,labels,mode,loss, model_weigths=None, show=False\n",
    "        self.metrics.calc_metrics(preds=preds, labels=labels, mode=mode, loss=epoch_loss, model_weigths=self.learner.model.state_dict(), show=True)\n",
    "\n",
    "    def test(self,mode,name_test, model_weigths=None):\n",
    "        self.learner.model.load_state_dict(model_weigths)\n",
    "        self.learner.model.train(False)\n",
    "        dataloader = self.data.get_loader(mode)\n",
    "        preds = []\n",
    "        ids = []\n",
    "        for (X, x_id) in tqdm.tqdm(dataloader):\n",
    "            X = X.to(DEVICE)\n",
    "            y_hat = self.learner.predict(X)\n",
    "            ids.extend(x_id)\n",
    "            preds.extend((y_hat).float().tolist())\n",
    "        \n",
    "        file_test_submtion =  open(f'{name_test}.csv','w')\n",
    "        file_test_submtion.write('id,fake_prob\\n')\n",
    "        for idx,pred in zip(ids,preds):\n",
    "            file_test_submtion.write(f\"{idx},{pred[0]}\\n\")\n",
    "        print(f\"Test submission for {name_test} saved at {name_test}.csv\")\n",
    "\n",
    "\n",
    "    def run(self, n_epochs: int):\n",
    "        print(\"Starting training\")\n",
    "        for t in range(n_epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            self.one_epoch(mode='train')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(mode='validation')\n",
    "        print(\"Training done\")\n",
    "        \n",
    "    def run_test(self,name_test):\n",
    "        #Keep test at the end of training\n",
    "        metric = 'f1-score'\n",
    "        print(f\"Generating test probs with {metric} best model:\")\n",
    "        with torch.no_grad():\n",
    "            best_model = self.metrics.get_best_model(metric=metric)\n",
    "            self.test(mode='test', name_test=name_test, model_weigths=best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets\n",
    "audio_train = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/train', is_label=True)\n",
    "audio_teste = AudioDataset(data_dir='/home/gustavo/Projects/PAV/DEEPFAKE-COMPTETITION-PAV/audios/test', is_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "data =Data(batch_size=100, dataset_train=audio_train, dataset_test=audio_teste, do_split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner\n",
    "learner = Learner(input_size=(1,64,94))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/home/gustavo/anaconda3/envs/study/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 43/43 [00:14<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.80 - Recall 0.85 - Precision 0.84 - F1-Score 0.84 - Loss 8.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation -  Acuracy: 0.76 - Recall 0.90 - Precision 0.75 - F1-Score 0.82 - Loss 1.41\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.84 - Recall 0.89 - Precision 0.87 - F1-Score 0.88 - Loss 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation -  Acuracy: 0.81 - Recall 0.93 - Precision 0.80 - F1-Score 0.86 - Loss 0.54\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:15<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.85 - Recall 0.93 - Precision 0.85 - F1-Score 0.89 - Loss 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation -  Acuracy: 0.81 - Recall 0.94 - Precision 0.79 - F1-Score 0.86 - Loss 0.55\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:16<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.86 - Recall 0.93 - Precision 0.86 - F1-Score 0.89 - Loss 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation -  Acuracy: 0.82 - Recall 0.88 - Precision 0.83 - F1-Score 0.85 - Loss 0.48\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:15<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.88 - Recall 0.94 - Precision 0.88 - F1-Score 0.91 - Loss 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation -  Acuracy: 0.83 - Recall 0.95 - Precision 0.80 - F1-Score 0.87 - Loss 0.52\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:16<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.90 - Recall 0.95 - Precision 0.90 - F1-Score 0.92 - Loss 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation -  Acuracy: 0.84 - Recall 0.90 - Precision 0.84 - F1-Score 0.87 - Loss 0.50\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:15<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  Acuracy: 0.91 - Recall 0.96 - Precision 0.91 - F1-Score 0.93 - Loss 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:00<00:02,  3.15it/s]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(data=data, evaluator=evaluator, learner=learner, metrics=metrics)\n",
    "trainer.run(n_epochs=10)\n",
    "trainer.run_test(name_test='test_cnn_baseline')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all metrics\n",
    "metrics.save_best_model(all_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save only one metric\n",
    "metrics.save_best_model(all_metrics=False, metric='recall')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
